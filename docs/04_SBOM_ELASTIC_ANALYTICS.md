# Pipeline SBOM i analiza aplikacji z wykorzystaniem Elastic Stack — A\architektura środowiska (Jenkins, “toolbox”, Elasticsearch, Kibana)

W proponowanej architekturze rolę orkiestratora CI/CD pełni Jenkins, który po zbudowaniu aplikacji wykonuje dodatkowe etapy DevSecOps: generowanie SBOM oraz skanowanie bezpieczeństwa. Etapy te mogą być realizowane wewnątrz kontenera narzędziowego (tzw. toolbox), zawierającego m.in. narzędzia do SBOM i analizy podatności, takie jak Syft (do generowania SBOM) oraz Grype (do skanowania CVE)[1]. Przykładowo pipeline po skompilowaniu aplikacji uruchamia Syft w celu wygenerowania SBOM (np. w formacie CycloneDX JSON z listą zależności), a następnie Grype analizuje ten SBOM pod kątem znanych podatności – np. wykrywa obecność podatnej biblioteki log4j i przypisanych do niej CVE[1].

Wyniki tych kroków (SBOM oraz lista znalezionych podatności wraz z ich szczegółami) są następnie pakowane jako zdarzenia (dokumenty JSON) zawierające także metadane build (identyfikator build, nazwa aplikacji, wersja itp.) i wysyłane do centralnego magazynu danych opartego o Elasticsearch[2]. Dzięki takiemu podejściu każda realizacja pipeline generuje kilka zdarzeń – osobno dla SBOM, raportu podatności itp. – co umożliwia indeksowanie i analizę poszczególnych aspektów builda w Elastic. Elasticsearch pełni tu rolę platformy analitycznej podobnie jak w innych rozwiązaniach SIEM, gromadząc dane bezpieczeństwa z pipeline (SBOM, skany, logi) i umożliwiając ich przeszukiwanie, agregację oraz wizualizację[3].

**Jenkins** może przekazywać zdarzenia bezpośrednio do Elasticsearch, np. poprzez wywołanie API indeksowania dokumentów (analogicznie do mechanizmu HEC w Splunk) – jest to lekki, bezpośredni sposób “push” eliminujący potrzebę dodatkowych pośredników[4]. Alternatywnie pipeline może zapisywać artefakty JSON lokalnie (np. w katalogu ```.lab_out/)```, skąd zostaną one odczytane i wysłane do Elasticsearch przez agenta typu Filebeat lub Logstash. Oba podejścia zapewniają asynchroniczny przepływ danych – Jenkins po wysłaniu zdarzeń może kontynuować pracę, podczas gdy Elastic Stack niezależnie je indeksuje i udostępnia do analizy w czasie rzeczywistym. Zwizualizowanie wyników umożliwia Kibana, gdzie można budować interaktywne dashboardy bezpieczeństwa i monitorować kluczowe metryki procesu (podobnie jak robi się to w Splunk)[3]. Dzięki integracji tych komponentów uzyskujemy spójne środowisko: Jenkins generuje i dostarcza dane, Elasticsearch je przechowuje i analizuje, a Kibana prezentuje je w przyjaznej formie. Całość może działać on-premises (dla pełnej kontroli nad danymi) lub w modelu hybrydowym/chmurowym, zależnie od wymagań organizacji.

## Kanał danych: typy zdarzeń SBOM i artefakty

Pipeline wysyła do Elasticsearch uporządkowany strumień zdarzeń JSON opisujących poszczególne fazy analizy SBOM. Każdy uruchomienie pipeline generuje kilka zdarzeń – m.in. osobno dla SBOM i dla raportu podatności – wraz z polami identyfikującymi kontekst builda (aplikacja, wersja, ID build)[2]. W ramach przyjętego schematu wyróżniono następujące typy zdarzeń (event_type):

- **```sbom_snapshot```** – migawka poprzedniego stanu SBOM. Zdarzenie to reprezentuje SBOM poprzedniej wersji aplikacji i służy jako baza do porównania (delta) z bieżącym wydaniem. Może zawierać identyfikator i metadane poprzedniego SBOM, ewentualnie skrót artefaktu z poprzedniego builda dla weryfikacji spójności.

- ```**sbom**``` – właściwy SBOM bieżącego builda. Zdarzenie to zawiera dane obecnej listy komponentów aplikacji (np. nazwy bibliotek, wersje, hashe, informacje o licencjach itp.). W zależności od konfiguracji może zawierać pełny spis komponentów (FULL_PAYLOAD) lub jedynie odniesienie/skrót do pliku SBOM. Zawsze natomiast zawiera podstawowe meta-informacje (np. liczba komponentów, suma kontrolna artefaktu aplikacji).

- ```**scan**``` – wynik skanowania podatności dla bieżącego SBOM. Zawiera listę wykrytych podatności (np. CVE) wraz z atrybutami takimi jak identyfikator CVE, poziom surowości (severity), opis, dostępne poprawki itp. oraz metryki zbiorcze (np. liczba podatności krytycznych, wysokich, średnich). Podobnie jak SBOM, może zawierać pełny payload (lista wszystkich znalezisk) albo tylko podsumowanie, zależnie od ustawień.

- ```**delta**``` – zestawienie zmian w składzie aplikacji w porównaniu do poprzedniego builda. Zdarzenie to wskazuje deltę komponentów: które zależności dodano, usunięto lub zaktualizowano. Przykładowo, jeśli dodano nową bibliotekę X w wersji Y, delta zawiera wpis o nowym komponencie; analogicznie dla usuniętych/zmienionych. Na podstawie delta można ocenić dryf architektoniczny – czy liczba zależności rośnie z wydania na wydanie oraz czy pojawiają się niespodziewane komponenty[5].

- ```**gate*``` – wynik oceny polityki bezpieczeństwa (bramki) dla danego builda. To zdarzenie podsumowuje, czy build przeszedł bramki czy został zablokowany, wraz z informacją dlaczego. Zawiera np. status status: PASS/FAIL, zastosowany próg (np. FAIL_ON=high), oraz zestawienie przekroczeń polityki (np. liczba krytycznych podatności vs. dopuszczalna). Stanowi swoisty alert wewnętrzny pipeline – dokumentuje decyzję o przepuszczeniu lub zatrzymaniu artefaktu.

Wszystkie powyższe zdarzenia są powiązane wspólnymi identyfikatorami, co umożliwia ich korelację. W szczególności używany jest zakres pól ```aid.*``` (skrót od application ID lub analysis ID), który obejmuje np. aid.application (nazwa aplikacji/projektu), aid.version (wersja aplikacji), aid.build_id (identyfikator builda/CI) itp. – dzięki temu każde zdarzenie zawiera tożsamość kontekstu, w jakim powstało. Na podstawie tych pól można w Kibanie łatwo filtrować i grupować dane dotyczące konkretnego projektu czy nawet pojedynczego uruchomienia pipeline. Na przykład wszystkie zdarzenia z jednym aid.build_id odnoszą się do jednego wykonania pipeline (jednego release’u), a aid.application pozwala agregować dane dla całej aplikacji na przestrzeni wielu wydań.

**Artefakty** ```.lab_out/*```: W implementacji referencyjnej pipeline Jenkins zapisuje wygenerowane zdarzenia jako pliki JSON w katalogu roboczym (np. w folderze .lab_out/). Dla każdego kroku powstaje plik odpowiadający zdarzeniu (np. ```sbom.json```, ```scan.json```, ```delta.json```, ```gate.json)```. Takie podejście ułatwia debugging (mamy kopie danych na dysku) oraz pozwala zastosować Filebeat/Logstash do odczytu tych plików i wysyłki do Elasticsearch. Pliki te stanowią trwały log analizy dla danego builda. Poza wysłaniem do Elastic, mogą być też archiwizowane (np. do repozytorium artefaktów) jeśli organizacja chce zachować pełne SBOM-y i raporty dla audytu. Zaletą archiwizacji lokalnej jest możliwość późniejszego odtworzenia stanu – np. gdyby zaszła potrzeba ponownej analizy lub weryfikacji, co dokładnie pipeline wykrył w danym wydaniu.

## Logika bramek decyzyjnych (GATE) i parametru FAIL_ON

Mechanizm **bramek bezpieczeństwa (Quality Gates)** w pipeline decyduje o zatrzymaniu lub przepuszczeniu artefaktu na podstawie wyników skanowania SBOM. Organizacja definiuje politykę określającą progi tolerancji ryzyka – np. “blokuj build, jeśli znajdzie się jakakolwiek podatność o surowości Critical” albo “dopuszczalne maksimum to 0 krytycznych i 5 wysokich podatności”[6]. Pipeline automatycznie porównuje rezultaty skanu z tymi progami i podejmuje decyzję: jeśli wynik przekracza dopuszczalne wartości, build zostaje oznaczony jako nieudany i przerywa się dalsze etapy (np. deployment na środowiska). W ten sposób CI/CD staje się strażnikiem polityki bezpieczeństwa – żadna wersja niespełniająca wymogów nie trafi na produkcję bez dodatkowej akceptacji.

Technicznie, implementacja bramek może wykorzystywać mechanizmy wbudowane w narzędzia skanujące lub własne skrypty. W praktyce często stosuje się parametr ```--fail-on``` oferowany przez skanery SCA (np. Grype, Snyk) – umożliwia on ustawienie progu, od którego narzędzie zwraca błąd i tym samym powoduje fail joba. Przykładowo, uruchomienie Grype z ```--fail-on critical``` spowoduje zakończenie procesu z kodem błędu, jeśli wykryto co najmniej jedną podatność Critical. Alternatywnie pipeline może wczytać wygenerowany raport (JSON) i samodzielnie wykonać walidację – skrypt parsuje raport i jeśli np. ```critical_count > 0``` (lub ```high_count``` przekracza ustalony limit), wtedy rzuca wyjątek/ustawia status fail. Oba podejścia są równoważne – ważne, by logika ta była niezwłoczna i nie dopuściła do wdrożenia artefaktu z nieakceptowalnym ryzykiem. Informacja o przekroczeniu progu jest również logowana – pipeline tworzy zdarzenie ```gate``` z opisem naruszenia polityki (np. *policy_violation: "CRITICAL_VULN_DETECTED"*) i wysyła je do Elastic, co pozwala na centralny wgląd w historię decyzji gate.

Parametr **FAIL_ON** można skonfigurować na różne poziomy: typowo opcje to ```critical```, ```high``` lub ```none```. Ustawienie ```FAIL_ON=critical``` oznacza, że już pojedyncza podatność o krytycznej surowości spowoduje przerwanie pipeline (mniej istotne luki nie wstrzymają procesu). ```FAIL_ON=high``` rozszerza kryterium na podatności wysokiej surowości (tj. każda luka High lub Critical zablokuje build). Z kolei ```FAIL_ON=none``` wyłącza automatyczne blokowanie – **pipeline nie przerywa się niezależnie od wykrytych luk**, a bramki działają w trybie monitoringu/ostrzegania. Ten ostatni tryb bywa stosowany we wczesnej fazie wdrożenia, aby zbierać dane i kalibrować polityki bez wpływu na ciągłość CI/CD. Rekomenduje się, aby **w fazie pilotażowej** nowego procesu ustawić bramki w tryb “warn-only” (FAIL_ON=none) – wszystkie zdarzenia o lukach są co prawda rejestrowane (jako ostrzeżenia), ale buildy nie są zatrzymywane. Pozwala to ocenić, jak często pojawiałyby się blokady, czy reguły nie generują false-positive itp. Gdy system dojrzeje i polityki zostaną dopracowane, można zaostrzyć działanie – np. globalnie włączyć FAIL_ON=high lub critical, egzekwując już twarde blokowanie buildów niezgodnych z polityką.

W praktyce po okresie kalibracji większość organizacji decyduje się na rygorystyczne egzekwowanie bramek. Na przykład ustala się, że żaden build zawierający podatność Critical nie przejdzie dalej, chyba że nastąpi formalne wyłączenie reguły dla danej luki. Taki proces wyjątków pozwala świadomie zaakceptować ryzyko w szczególnych przypadkach: odpowiednie role (np. AppSec i właściciel produktu) mogą udzielić zgody na tymczasowe dopuszczenie podatności, dodając ją do **allowlisty CVE** w konfiguracji pipeline. Pipeline rozpoznaje wtedy tę podatność jako wyłączoną z reguł i nie blokuje builda, ale fakt ten jest logowany (w zdarzeniu gate odnotowuje się np. exception: true, cve_allowlist: [*CVE-2021-XYZ*]). Dzięki temu zachowana jest transparentność – każda akceptacja ryzyka jest udokumentowana, a metryki (np. liczba otwartych wyjątków) można śledzić centralnie.

Podsumowując, logika bramek GATE sprowadza się do zamiany wiedzy o podatnościach w automatyczną decyzję na poziomie pipeline. SBOM i skan dostarczają sygnał (np. “znaleziono 2 krytyczne luki”), który porównujemy z progiem (polityką bezpieczeństwa). Jeśli sygnał przekracza próg – następuje akcja: fail builda, zablokowanie wdrożenia i ewentualnie powiadomienie zespołów[7]. Takie sprzężenie zwrotne sprawia, że bezpieczeństwo staje się integralną częścią procesu dostarczania oprogramowania, a ryzyka są wychwytywane wcześnie (już na etapie build) zamiast dopiero w produkcji[7].

*(Uwaga: Warto dodać, że początkowe “miękkie” podejście (FAIL_ON=none) ma sens nie tylko dla kalibracji, ale też by oswoić deweloperów z nowym procesem. Pozwala to uniknąć frustracji nagłym pojawieniem się czerwonych buildów – najpierw widzą ostrzeżenia i mają szansę poprawić kwestie bezpieczeństwa, zanim pipeline zacznie je egzekwować surowo. Po przełączeniu bramek w tryb fail, należy jasno zakomunikować zespołom zasady i zapewnić wsparcie w analizie raportów, aby feedback loop był pozytywny.)*

## Przykładowe pytania analityczne (SBOM & Elastic)

Dysponując zintegrowanymi danymi SBOM w Elasticsearch, można zadawać zaawansowane pytania analityczne dotyczące trendów i zależności. Oto kilka przykładów takich pytań i analiz:

- **Jakie komponenty zostały dodane lub usunięte między kolejnymi wersjami?**

    Dzięki wydarzeniom typu ```delta``` można śledzić **zmiany w składzie aplikacji***. Analiza delt ujawnia przyrost lub ubytek zależności: np. w wersji 2.0 dodano 3 nowe biblioteki (X, Y, Z) i usunięto 1 (A). Pozwala to wykryć **dryf architektoniczny** – czy projekt nie puchnie nadmiernie z release’u na release (ciągły przyrost zależności) oraz czy nie pojawił się jakiś nowy komponent nieobecny wcześniej[8]. Pytanie to pomaga ocenić, czy zmiany w SBOM są spodziewane (wynikające z zamierzonych aktualizacji), czy też zaskakujące (np. nieautoryzowana biblioteka). Z biznesowego punktu widzenia wskazuje też, czy rozszerza się powierzchnia ataku (każda nowa zależność to potencjalnie nowe podatności)[9].

- **Czy nowe podatności w danej wersji wynikają z nowo dodanych komponentów?**

    Innymi słowy, czy korelacja między deltą a skanem wskazuje, że ryzyko wzrosło przez wprowadzenie nowych zależności. Można to zbadać krzyżując zdarzenia delta i scan: jeśli w danym wydaniu pojawiła się nowa podatność CVE-XYZ o wysokiej surowości oraz delta wskazuje dodanie komponentu zawierającego tę podatność, to można z dużym prawdopodobieństwem stwierdzić, że właśnie ten nowy komponent przyniósł lukę. Przykład: w wersji 2.0 dodano bibliotekę Log4j 2.14.1 i raport scan wykazał krytyczną lukę CVE-2021-44228 – korelacja pokazuje, że dodanie Log4j spowodowało pojawienie się podatności Log4Shell. Taka analiza wskazuje przyczynę wzrostu ryzyka i pozwala szybko namierzyć winowajcę (bibliotekę do ewentualnego usunięcia lub aktualizacji). Analogicznie, jeśli jakieś nowe luki pojawiły się mimo braku zmian w zależnościach, może to oznaczać np. wykrycie CVE dla istniejącego wcześniej komponentu (nowe CVE w bazach) – co rodzi inne działania (np. aktualizacja komponentu, który dotąd uchodził za bezpieczny).

- **Czy nastąpiła regresja bezpieczeństwa w nowym wydaniu?**

    To pytanie sprowadza się do porównania poziomu ryzyka między kolejnymi wersjami. Regresja oznacza pogorszenie – np. większą liczbę lub wyższą surowość luk w porównaniu do poprzedniego release’u. Dzięki danym historycznym w Elasticsearch można automatycznie wyliczać metryki typu delta ryzyka. Jeśli wersja 2.0 ma więcej podatności niż 1.0 (np. doszły 2 krytyczne luki), jest to sygnał regresji bezpieczeństwa. Szczególnie interesuje nas, które wydanie pogorszyło stan – np. “wydanie 1.3 spowodowało wzrost liczby luk z 5 do 15 (w tym pojawienie się krytycznych)” – oraz jakie konkretnie zmiany do tego doprowadziły (korelacja z deltą jak wyżej). Taka wiedza pozwala wyciągać wnioski co do jakości zmian wprowadzanych przez zespoły (czy np. dodając pewną funkcjonalność wnieśli też dług bezpieczeństwa). Co więcej, analizując wiele projektów, można zidentyfikować wspólne wzorce regresji (np. czy określony typ zmian – jak duże upgrade’y frameworków – zwykle wiąże się ze wzrostem liczby luk).

- **Jak zmienia się trend ryzyka w czasie?**

    Tutaj patrzymy na długofalowe tendencje: czy ogólna liczba podatności (lub ich łączna krytyczność) w projekcie maleje, rośnie, czy pozostaje stała w kolejnych iteracjach. Na podstawie danych z kolejnych scan można zbudować wykres trendu: np. liczba podatności krytycznych i wysokich w projekcie w kolejnych sprintach. Idealnie, trend powinien być malejący lub zerowy (co świadczy o systematycznym spłacaniu długu bezpieczeństwa). Jeśli natomiast z każdą wersją przybywa nowych luk, oznacza to narastający problem. Przykładowo, organizacja może śledzić metrykę “liczba podatności High+Critical na release” – jeśli w jednym wydaniu było 10 (w tym 2 krytyczne), a w następnym już 2 (i żadnej krytycznej), to jest to trend pozytywny sugerujący redukcję ryzyka[10]. Wizualizacja takiego trendu (np. wykres liniowy) szybko pokazuje, czy zmierzamy w dobrą stronę. Dodatkowo można liczyć procent “czystych” buildów (bez żadnych ostrzeżeń) na przestrzeni czasu[11] – rosnący odsetek oznacza poprawę procesu. Trendy ryzyka pozwalają zadać pytania strategiczne: czy nasze działania security (np. refaktoryzacja zależności, update’y) przynoszą efekt?; czy tempo poprawy jest zadowalające?; czy może potrzebujemy dodatkowych inicjatyw, bo trend jest płaski lub negatywny?.

Inne pytania analityczne, które można badać, to np.: która biblioteka odpowiada za najwięcej luk w projekcie (identyfikacja “najbardziej ryzykownego” komponentu), jak szybko zespół reaguje na wykryte luki (metryka MTTR – średni czas od wykrycia podatności do wydania wersji z jej usunięciem), czy pewne podatności utrzymują się przez wiele wydań (czyli czy są ignorowane – jeśli ta sama krytyczna luka jest obecna w 3 kolejnych release’ach, to znaczy że zespół ją odkłada, co może wymagać eskalacji)[12]. Wszystkie te analizy stają się możliwe dzięki temu, że SBOM i wyniki skanów traktujemy nie jako pojedynczy raport, ale jako ciągły strumień danych, który możemy przekroić na wiele sposobów. DevSecOps w połączeniu z Elastic umożliwia zatem podejście data-driven do bezpieczeństwa: zamiast intuicji, mamy twarde dane i metryki, które możemy śledzić i poprawiać iteracyjnie.

## Rozszerzenie analizy o błędy runtime (logi aplikacji)

SBOM i skany podatności opisują **statyczny stan bezpieczeństwa** artefaktu (tj. czego używamy i z jakimi lukami się to wiąże). Jednak równie ważne dla pełnego obrazu jest to, jak aplikacja zachowuje się podczas wykonywania (runtime) – czy generuje błędy, czy wykorzystuje podatne fragmenty kodu, czy jest atakowana itp. Rozbudowanie strumienia danych o **logi aplikacyjne (szczególnie logi błędów)** pozwala powiązać informacje SBOM z realnymi symptomami działania systemu. SBOM może nam powiedzieć, że aplikacja zawiera komponent X, ale nie powie, czy w produkcji faktycznie korzystamy z funkcjonalności X ani czy ktoś próbuje ją zaatakować – te informacje pochodzą z obserwacji runtime (logi, monitoring)[13]. Integrując oba źródła, zyskujemy znacznie bogatszy kontekst.

**Jak włączyć logi błędów do strumienia?** Jednym ze sposobów jest skonfigurowanie zbierania logów z uruchomionej aplikacji (np. z środowiska testowego lub produkcyjnego) do Elasticsearch – np. przy użyciu Filebeat z modułem dla logów aplikacji Java/Python itp. Logi te powinny zawierać znaczniki pozwalające powiązać je z konkretną aplikacją i wersją (np. tag z nazwą aplikacji/wersją wdrożenia, aby w Elastic można je skorelować z odpowiednim SBOM). W praktyce, jeśli pipeline po zbudowaniu artefaktu uruchamia go w ramach testów integracyjnych, można w tym kroku zebrać logi i wysłać do Elastic – z polami ```aid.application``` i ```aid.version``` odpowiadającymi danemu buildowi.

**Korelacja błędów z komponentami SBOM** polega na łączeniu informacji o tym gdzie wystąpił błąd z tym z czego zbudowana jest aplikacja. Na przykład, jeśli w logu pojawia się wyjątek ```NullPointerException``` w ```klasie com.example.libX.Helper``` (należącej do biblioteki libX), to mając SBOM, możemy ustalić, że klasa ta pochodzi z komponentu libX w wersji np. 1.2. W Elastic można to osiągnąć poprzez wzbogacenie zdarzeń logów o informację o komponencie: np. podczas ingestii logu uruchomić procesor (lub skrypt) sprawdzający, do jakiego pakietu/jar-a należy dany wpis stack trace. Innym podejściem jest wcześniejsze przygotowanie mapy: dla każdej biblioteki z SBOM znamy charakterystyczny prefiks pakietów (np. ```com.example.libX``` -> libX), co pozwala programatycznie tagować logi. W wyniku takiej korelacji, do każdego zdarzenia logu błędu możemy dodać pole ```component: libX:1.2```.

**Metryki error_rate per komponent**: Gdy już błędy mają przypisane komponenty, możemy wyliczać statystyki awaryjności bibliotek. Na przykład: ile błędów (exceptionów) generuje każda biblioteka na 1000 transakcji lub jaki procent wszystkich error logów jest związany z danym komponentem. Metrka error_rate dla komponentu to liczba zarejestrowanych błędów pochodzących z tego komponentu (w jednostce czasu lub na jednostkę ruchu). Jeśli dany komponent pojawia się w logach błędów znacznie częściej niż inne, może to wskazywać na jego niską jakość, problemy z kompatybilnością lub błędną integrację. Przykładowo, możemy odkryć, że biblioteka FooLib 3.1 wywołała 50 błędów w ciągu ostatniego tygodnia, co stanowi 40% wszystkich błędów aplikacji – to sygnał, że FooLib jest wadliwym elementem i warto rozważyć jej aktualizację lub zastąpienie. Z kolei jeśli jakaś podatna biblioteka nigdy nie pojawia się w logach (jej funkcje nie są używane), to choć formalnie ma CVE, praktycznie ryzyko jej wpływu jest mniejsze – co przypomina koncepcję VEX/“not affected”.

Łącząc dane SBOM z logami runtime uzyskujemy również wgląd w eksploatację podatności. Jeżeli w logach bezpieczeństwa (np. z IDS/IPS, WAF) pojawiają się wzmianki o atakach wykorzystujących znaną lukę, i jednocześnie SBOM wskazuje, że aplikacja ma podatny komponent, możemy dokonać korelacji: potwierdzić, że dana podatność jest realnie atakowana[14][13]. Z drugiej strony, jeśli mamy podatność w SBOM, ale logi aplikacyjne ani ruch sieciowy nie wskazują na próby wykorzystania (brak błędów ani alertów związanych z tą luką), to możemy ocenić priorytet takiej luki jako nieco niższy[15]. Takie podejście redukuje szum – filtruje te podatności, które mimo istnienia technicznego mogą nie stanowić praktycznego zagrożenia, bo kod nigdy nie jest wykonywany. Oczywiście, brak oznak w logach nie oznacza, że lukę można całkiem zignorować, ale pozwala mądrzej priorytetyzować (najpierw naprawiamy to, co generuje błędy lub co ktoś próbuje atakować tu i teraz).

Z punktu widzenia Kibany, możemy zbudować dashboard jakości runtime, na którym znajdą się np.: wykres błędów w czasie (globalnie), lista Top 5 komponentów generujących najwięcej wyjątków, mapa cieplna błędów względem modułów aplikacji, czy korelacja wersja komponentu vs liczba błędów (co może ujawnić, że np. wersja 2.0 biblioteki powoduje 2x więcej błędów niż 1.0). Tego typu analizy wykraczają poza klasyczny SBOM (skupiony na podatnościach) i wchodzą w obszar obserwowalności i niezawodności aplikacji – dając pełniejszy obraz zdrowia aplikacji.

## Projekt dashboardu Kibana (przykładowe panele)

Aby w przejrzysty sposób prezentować zebrane dane SBOM i ich analizę, można zbudować w Kibanie dedykowany dashboard składający się z kilku uzupełniających się paneli. Poniżej opisujemy proponowane komponenty takiego dashboardu wraz z ich rolą:

- **Timeline bramek (Gate timeline)** – Panel ten prezentuje oś czasu uruchomień pipeline (np. kolejne buildy w porządku chronologicznym) z zaznaczeniem, które z nich przeszły bramki, a które zostały zablokowane. Można to zrealizować np. jako wykres kategoryczny: oś X – czas (albo numer builda), oś Y – status (PASS/FAIL), ewentualnie zaznaczone kolorem (zielony = PASS, czerwony = FAIL). Dzięki temu szybko dostrzeżemy okresy, w których następowały serie nieudanych buildów z powodu polityk bezpieczeństwa. Panel może też pokazywać odsetek buildów zablokowanych vs przepuszczonych w wybranym zakresie czasu[16] – co jest miarą ogólnej “zdrowotności” projektu w kontekście polityk (idealnie ten odsetek z czasem maleje, gdy zespoły na bieżąco eliminują problemy). Timeline bramek pełni rolę wczesnego ostrzegania: jeżeli widzimy kilka czerwonych punktów z rzędu, oznacza to, że ostatnie zmiany ciągle nie spełniają wymagań (np. nadal obecna krytyczna luka).

- **Trend krytycznych/wysokich podatności** – Jest to wykres (np. liniowy) obrazujący zmianę liczby najpoważniejszych podatności w czasie. Dla każdej kolejnej wersji (builda) wyliczamy ile miał podatności o surowości High i Critical – i wykreślamy to jako dwie serie na wspólnym wykresie. Otrzymujemy trend, który pozwala odczytać, czy sytuacja się poprawia. Przykładowo linia “Critical” może od pewnego momentu spaść do zera i pozostać na zerze (co oznacza, że od czasu usunięcia wszystkich krytycznych luk żadna nowa się nie pojawiła), linia “High” może sukcesywnie opadać ze, powiedzmy, 10 do 2 na przestrzeni kilku wydań[10]. Taki panel odpowiada na pytanie “czy redukujemy najwyższe ryzyka?”. Można też zamiast liczby luk użyć skumulowanej oceny ryzyka – np. sumy wag (Critical=5, High=4, Medium=3…) dla wszystkich luk w release – aby mieć pojedynczą metrykę ryzyka w czasie. Trend rosnący byłby alarmujący, malejący – pożądany. Panel trendu dobrze oddziałuje na wyobraźnię zespołów i managementu: pokazuje dług bezpieczeństwa jak na dłoni (rośnie czy maleje).

- **Wolumen zmian (Delta)** – Ten panel skupia się na skalę zmian w składzie aplikacji między wersjami. Można go zrealizować jako wykres słupkowy: oś X – kolejne wydania, oś Y – liczba zmian w SBOM. Każdy słupek mógłby być podzielony (stacked) na dwie części: liczba dodanych komponentów vs usuniętych komponentów w danym wydaniu. Alternatywnie można pokazać osobno dwie linie: new components per release i removed components per release. Panel ten od razu pokazuje, które releasy były dużymi skokami architektonicznymi (wysoki wolumen zmian), a które były stabilne. Jeśli np. wersja 2.0 aplikacji dodała 10 nowych bibliotek – słupek będzie wysoki, co sygnalizuje potencjalnie większe ryzyko (bo wraz z nowymi bibliotekami zwykle pojawiają się nowe podatności). Panel delty ujawnia też nietypowe anomalia: np. nagłe pojawienie się wielu nowych komponentów w drobnej wersji może budzić pytania (czy to na pewno zamierzone?)[8]. Ten panel sprzyja rozmowie architektonicznej – czy kierunek zmian jest zgodny z planem (np. eliminujemy stare zależności, czy raczej ciągle dokładamy nowe?).

- **Heatmapa zdarzeń bezpieczeństwa** – Proponowany panel typu heatmap (mapa cieplna) służy do wychwytywania okresowości i natężenia zdarzeń w czasie. Jednym zastosowaniem jest kalendarzowe przedstawienie liczby zdarzeń (np. liczby podatności wykrytych) w układzie dzień vs godzina – co może ujawnić, kiedy zwykle pojawiają się problemy (np. tuż przed wydaniem releasu, w poniedziałki rano itp.). Innym zastosowaniem może być macierz: oś X – typ zdarzenia (SBOM, scan, delta, gate), oś Y – dzień tygodnia, kolory reprezentują częstość. To mogłoby pokazać np., że zdarzenia typu gate FAIL koncentrują się w piątki (co może sugerować, że ostatnie commity tygodnia często wprowadzają krytyczne luki, a buildy piątkowe się wywalają). Heatmapa umożliwia szybkie zlokalizowanie hot-spotów, czyli pików zdarzeń, które warto dalej zbadać. Dla managementu przydatna bywa też heatmapa ryzyka w czasie – np. oś X: kwartały roku, oś Y: projekty, kolor: średnia liczba wysokich+krit w projekcie – co jednym rzutem oka pokazuje, które projekty i kiedy były w złym stanie (ciemnoczerwone pola), a gdzie jest poprawa (zieleni się). Panel typu heatmap daje dużą gęstość informacji i sprzyja korelacjom wzrokowym.

- **Tablica przebiegów (lista buildów)** – Ostatnim elementem jest tabelaryczne zestawienie kluczowych danych dla każdego builda (wydania). Taki panel wyświetla np. kolumny: Build ID, Data, Wersja aplikacji, Liczba komponentów, Liczba podatności (Critical/High), Status bramek. Każdy wiersz to jeden build (można filtrować po aplikacji/projekcie). Tablica przebiegów pełni funkcję szczegółowego podglądu – umożliwia szybkie odniesienie metryk do konkretnego wydania. Np. można łatwo znaleźć “w którym buildzie pojawiła się podatność CVE-2022-12345?” albo “kiedy liczba komponentów przekroczyła 100?”. W Kibanie realizuje się to najczęściej wizualizacją typu Data Table z odpowiednią agregacją (np. Terms po build_id, z metrykami max(data), sum(podatności) itp.). Taka tabela bywa też pierwszym punktem wejścia do analizy ad-hoc – użytkownik może kliknąć konkretny build i dostać jego szczegóły (np. w Discover zobaczyć pełny dokument SBOM/scan dla danego wydania).

Wspólnie powyższe panele dostarczają pełnej narracji: od ogólnego trendu ryzyka, poprzez historię decyzji bramek, aż po szczegóły konkretnych wydań. Taki dashboard stanowi centralny pulpit dla DevSecOps – umożliwia zarówno inżynierom jak i managerom szybkie zorientowanie się w stanie bezpieczeństwa projektu. Co istotne, każda wizualizacja opiera się na danych z indeksu Elastic, więc można je dynamicznie filtrować (np. wybierając zakres dat, konkretną aplikację, środowisko itd.). Dzięki temu różne role mogą wyciągać z dashboardu interesujące ich informacje: deweloper sprawdzi, czy jego ostatni build przeszedł bezpiecznie, AppSec zobaczy trend podatności, a menedżer – czy ryzyko w projekcie jest pod kontrolą.

## Przykładowe scenariusze (rozwój repozytorium a dane SBOM)

Aby zilustrować, jak pipeline SBOM generuje dane i jak można je interpretować, rozważmy kilka scenariuszy zmian w projekcie, które wpłyną na metryki bezpieczeństwa (delta, ryzyko, decyzje gate):

- **Dodanie nowej zależności podatnej** – Załóżmy, że deweloper dołącza do projektu nową bibliotekę, np. Log4j w wersji 2.14.1, aby skorzystać z logowania. Ta zmiana powoduje, że w SBOM-ie pojawia się nowy komponent (Log4j:2.14.1), co zostanie odnotowane w zdarzeniu delta (jako added_component). Po zbudowaniu pipeline uruchamia skan Grype – który wykrywa słynną podatność Log4Shell (CVE-2021-44228) powiązaną z tą wersją Log4j[17]. Zdarzenie scan zawiera tę krytyczną lukę (severity: Critical, CVE-2021-44228). Ponieważ polityka bezpieczeństwa ma FAIL_ON=critical, pipeline generuje zdarzenie gate z wynikiem FAIL i szczegółem, że przekroczono próg (znaleziono krytyczną podatność)[7]. W efekcie build zostaje zablokowany – biblioteka nie może wejść na środowisko dopóki problem nie zostanie rozwiązany. Na dashboardzie Kibany zobaczymy dla tego builda czerwony znacznik (fail), wysoki słupek “+1” w panelu delta (nowy komponent) oraz pojawienie się punktu na wykresie trendu krytycznych (w górę o +1). Taki scenariusz pokazuje, jak nowa zależność wprowadziła ryzyko i jak pipeline od razu to wychwycił.

- **Dodanie złośliwego pakietu** – Rozważmy inny przypadek: developer (nieświadomie) dodaje zależność do pakietu, który okazuje się złośliwy (np. zhakowana biblioteka z npm o nazwie event-stream albo biblioteka ze złośliwą zawartością typu malware). SBOM odnotuje nowy komponent, ale co ze skanem? Jeśli pakiet nie ma przypisanego CVE (bo np. nie jest to oficjalna podatność, tylko backdoor), typowe skanery SCA mogą go nie oznaczyć jako lukę. Jednak taki komponent może zostać wykryty przez mechanizmy bezpieczeństwa innego typu – np. skanery malware, systemy reputacyjne lub reguły polisy (np. “zakaz używania pakietów spoza zaufanego registry”). Załóżmy, że nasz pipeline ma integrację z bazą złośliwych pakietów i rozpoznaje, że nowo dodany komponent znajduje się na czarnej liście. Wówczas zdarzenie scan mogłoby zawierać wpis typu “malicious_package_detected: true, name: <nazwa pakietu>” z wysokim priorytetem. gate w rezultacie oceni to jako naruszenie polityki (niekoniecznie CVE, ale np. FAIL_ON=high obejmie też krytyczne alerty jakości) i zatrzyma build. W dashboardzie zobaczymy znów, że konkretny build został zablokowany, choć liczba CVE może się nie zmienić – natomiast np. liczba “policy violations” wzrośnie. Jeśli pipeline nie miał automatycznego wykrywania tego typu zagrożeń, incydent i tak pozostanie widoczny w danych SBOM – nowy komponent pojawi się w SBOM i będzie odstawał (np. nie występuje w innych projektach, ma podejrzaną nazwę). Analityk bezpieczeństwa przeglądający zestawienie zależności mógłby ręcznie zauważyć niepożądany pakiet. Ten scenariusz unaocznia, że SBOM służy nie tylko wykrywaniu znanych CVE, ale też kontroli składu – każdy nieplanowany lub podejrzany komponent jest sygnałem do inspekcji.

- **Wprowadzenie patcha (aktualizacja komponentu)** – W trzecim scenariuszu zespół reaguje na wykryte ryzyko i aktualizuje wadliwą bibliotekę do bezpiecznej wersji. Powróćmy do przypadku Log4j: developer podbija wersję z 2.14.1 do 2.17.0, w której dziura Log4Shell jest załatana. Pipeline wygeneruje SBOM z nową wersją komponentu (Log4j:2.17.0 zamiast 2.14.1). W zdarzeniu delta pojawi się informacja, że komponent log4j został zaktualizowany (stara wersja usunięta, nowa dodana). Skan Grype tym razem nie znajdzie już podatności CVE-2021-44228 – co więcej, ogólna liczba podatności spadnie (usunęliśmy jedną krytyczną). scan może nadal wykazać jakieś inne luki, ale kluczowe jest, że krytyczna zniknęła. Zdarzenie gate najprawdopodobniej pokaże PASS (o ile nie było innych naruszeń) – build przejdzie pomyślnie. W efekcie na wykresie trendu zobaczymy spadek liczby krytycznych z 1 do 0, co jest pozytywną delta ryzyka[10]. W timeline bramek nie będzie czerwonego punktu – problem został rozwiązany. Taka sytuacja stanowi pozytywny przykład: pokazuje, jak działania naprawcze przekładają się na mierzalną poprawę bezpieczeństwa. W praktyce można zweryfikować, czy po wdrożeniu patcha kolejny release faktycznie był “czysty” – jeśli tak, to znaczy że proces DevSecOps zadziałał skutecznie (luka nie trafiła na produkcję). Analiza historyczna kilku takich przypadków może pomóc oszacować, ile czasu zajmuje zespołowi załatanie krytycznej luki (czy od razu w następnym sprincie, czy dopiero po kilku) – co z kolei jest cenną metryką do doskonalenia procesu (dążymy do minimalizacji tego czasu).

Podsumowując, powyższe scenariusze obrazują generowanie “ciekawych danych”: dodanie zależności zwiększa powierzchnię ataku i może ujawnić nowe CVE, dodanie złośliwego komponentu stanowi zagrożenie innego rodzaju, a wycofanie/naprawa komponentu zmniejsza ryzyko. Te zdarzenia będą widoczne w indeksie Elastic i na dashboardach: jako skoki w liczbie podatności, zmiany statusów gate, wychylenia na wykresach trendów itp. Dzięki temu zespół może uczyć się na bieżąco – np. “ostatnio dodaliśmy bibliotekę X i od razu mieliśmy problemy – następnym razem zwróćmy większą uwagę na ocenę komponentu przed dodaniem”. W ten sposób SBOM staje się nie tylko statycznym raportem, ale żywym strumieniem wiedzy napędzającym doskonalenie procesu tworzenia oprogramowania.

## Indeksowanie pełnych danych SBOM (FULL_PAYLOAD=true) a struktura zdarzeń

W trakcie wdrożenia pojawia się decyzja, czy indeksować w Elasticsearch pełne dane SBOM i skanów (tzw. FULL_PAYLOAD), czy ograniczyć się do pól podsumowujących. Ustawienie FULL_PAYLOAD=true oznacza, że zdarzenia typu sbom i scan będą zawierały cały ładunek informacji: listę wszystkich komponentów (w sbom) oraz listę wszystkich znalezionych podatności (w scan). Struktura dokumentu JSON staje się wtedy bardziej złożona – pojawiają się pola zawierające tablice obiektów, np. ```"components": [ { "name": "libA", "version": "1.2", ...}, { "name": "libB", ...}, ... ]``` lub ```"vulnerabilities": [ { "cve": "CVE-2022-12345", "severity": "High", ...}, {...} ]```. Z kolei przy FULL_PAYLOAD=false w zdarzeniach znajdują się jedynie informacje zagregowane: np. liczba komponentów, ewentualnie wybrane pola (jak nazwa aplikacji, wersja, licznik podatności krytycznych/wysokich). Szczegółowy SBOM może być wtedy przechowywany poza Elasticsearch (np. jako plik w repozytorium artefaktów).

**Zalety pełnego payloadu:** Bogatsze zdarzenia pozwalają na bardziej szczegółowe analizy bezpośrednio w Kibanie. Na przykład, mając zaindeksowane pełne SBOM-y, analityk może w kilka minut przeszukać całą infrastrukturę pod kątem konkretnego komponentu – np. znaleźć wszystkie aplikacje używające biblioteki log4j w dowolnej wersji[18]. To potężna możliwość, zwłaszcza w scenariuszach reagowania na nowe zagrożenia (zero-day): gdy pojawia się informacja o luce w popularnej bibliotece, możemy natychmiast zadać Elasticowi pytanie “gdzie wszędzie ona występuje?”. Jeśli SBOM-y są pełne i zindeksowane, odpowiedź przychodzi w sekundach, zamiast godzinnego przeszukiwania repozytoriów czy pytania wszystkich zespołów[18]. Podobnie, pełne raporty podatności umożliwiają agregację na poziomie pojedynczych luk – np. policzenie, która podatność pojawia się najczęściej w całym portfolio projektów[19]. Innym plusem jest możliwość budowania dynamicznych przekrojów: np. lista wszystkich komponentów (ze wszystkich SBOM) z ich wersjami i sumą wystąpień – co tworzy globalny katalog zależności organizacji. Krótko mówiąc, FULL_PAYLOAD=true maksymalizuje widoczność: Elastic staje się pełnym repozytorium wiedzy o komponentach i lukach.

**Wyzwania pełnego payloadu:** Indeksowanie tak szczegółowych dokumentów oznacza jednak, że rozmiar indeksu rośnie (czasem znacząco). Każdy komponent i każda luka mogą tworzyć dodatkowe pola w indeksie, co przy setkach projektów i częstych buildach może prowadzić do bardzo dużej liczby indeksowanych elementów. Należy zaprojektować mapping tak, aby uniknąć explosion pól – np. wykorzystać struktury nested zamiast dynamicznie tworzyć osobne pole dla każdego klucza w obiekcie. Dobrą praktyką jest zdefiniowanie pól komponentów i podatności jako pola z góry znanymi nazwami (name, version, cpe, cve, severity, itp.), w przeciwnym razie Elasticsearch przy pierwszym napotkaniu neznanego klucza utworzy nowe pole w mappingu. W standardowych formatach SBOM (CycloneDX, SPDX) klucze są stałe, więc to mniej problematyczne – gorzej, jeśli mamy niestandardowe atrybuty. Ponadto, zapytania na dużych tablicach mogą być wolniejsze – np. jeśli każda SBOM ma 200 komponentów, a chcemy filtrować po właściwościach komponentu, to Elasticsearch musi skanować dość złożone dokumenty. Pomocne jest tu użycie wspomnianych pól typu nested, które pozwalają efektywniej przeszukiwać tablice obiektów.

**Różnice w strukturze zdarzeń:** Przy FULL_PAYLOAD=false zdarzenia są prostsze, ale tracimy bezpośredni dostęp do szczegółów. Przykładowo, zdarzenie ```scan``` może zawierać pola: ```vuln_total=10```, ```critical_count=1```, ```high_count=3```, ... i ewentualnie listę CVE w formie uproszczonej (np. ```cve_list: ["CVE-2021-44228","CVE-2022-11111",...]```). Jeśli chcielibyśmy dowiedzieć się czegoś więcej o którejś z tych podatności, musielibyśmy zajrzeć do oryginalnego raportu (pliku) albo do osobnego systemu. Z kolei przy FULL_PAYLOAD=true, całe obiekty podatności są w dokumencie – więc poza podstawowymi polami mamy np. pełen opis CVE, wektor CVSS itp. Podobnie, w sbom bez pełnego payloadu możemy mieć np. tylko listę komponentów jako stringi czy sumę hash, podczas gdy z pełnym payloadem mamy każdy komponent z osobna. Strukturalnie dokumenty z FULL_PAYLOAD są głębsze (zawierają zagnieżdżone kolekcje), co wymaga uwzględnienia w mappingu i zapytaniach (np. używania zapytań typu nested query, by wyszukiwać po polach wewnątrz komponentów).

**Wybór podejścia zależy od priorytetów:** jeśli kluczowa jest szybkość zapytań i rozmiar indeksu, można rozważyć ograniczenie zakresu danych w dokumencie (np. trzymać pełny SBOM tylko poza Elastic, a w indeksie jedynie metadane i ewentualnie referencję do pliku). Wtedy, w razie potrzeby szczegółowej analizy, sięga się do zewnętrznego repozytorium SBOM. Wiele organizacji stosuje hybrydowe rozwiązanie: indeksuje w SIEM to, co potrzebne do typowych dashboardów i alertów, natomiast pełne SBOM-y zachowuje w repozytorium artefaktów (Nexus, Artifactory) lub dedykowanej bazie SBOM[20]. Daje to możliwość wersjonowania SBOM i późniejszych audytów, jednocześnie nie obciążając nadmiernie indeksu. Z drugiej strony, są przypadki (np. SOC, threat hunting), gdzie pełne SBOM-y w SIEM są ogromnym atutem i wtedy świadomie akceptuje się większy wolumen danych na rzecz możliwości natychmiastowego przeszukiwania zależności[18].

Reasumując, FULL_PAYLOAD=true czyni z Elastic pełnoprawne centrum analizy SBOM – co jest zgodne z ideą traktowania SBOM jako strumienia danych. Posiadanie pełnych danych pozwala na korelacje z innymi źródłami (TI, logi runtime) oraz szybkie reakcje na incydenty. Jeśli jednak infrastruktura nie pozwala na tak duże indeksy, kompromisem jest indeksowanie tylko części danych i trzymanie reszty offline. Ważne, by już na etapie projektowania mappingu podjąć tę decyzję i przygotować indeks pod kątem oczekiwanego użycia.

## Mapowanie pól i schemat indeksu ```sbom-test```

Aby efektywnie korzystać z danych SBOM w Elasticsearch, konieczne jest odpowiednie zaprojektowanie mapowania indeksu (index mapping). Kluczowe jest, by pola służące do filtrowania, agregacji czy łączenia danych miały właściwe typy (np. ```keyword``` dla identyfikatorów, ```date``` dla dat, ```integer``` dla liczników itp.). Nieprawidłowe mappingi mogą skutkować problemami w analizie – np. jeśli pole z nazwą komponentu będzie domyślnie typu text (analizowane), nie pogrupujemy poprawnie wyników po tej nazwie. Poniżej przedstawiono główne pola indeksu ```sbom-test``` i zalecane typy:

- Identyfikatory i kategorie: Pola takie jak ```event_type```, ```aid.applicatio```, ```id.version```, ```aid.build_id```, ```component.name```, ```component.version```, ```vulnerability.cve```, ```vulnerability.severity``` powinny być typu keyword. Dzięki temu możliwe jest dokładne filtrowanie i grupowanie po tych polach (np. agregacja terms po ```event_type``` czy po ```vulnerability.severity```). Gdyby pozostały typu text, Elastic rozbiłby ich wartości na tokeny (np. “Critical” jako “critical”, “CVE-2021-44228” na “CVE”, “2021”, “44228”), co utrudniałoby sensowne operacje. Keyword zachowuje oryginalną wartość w całości. Dotyczy to także pól zawierających np. nazwy bibliotek – chcemy traktować “spring-framework” jako pojedynczy token, a nie dwa słowa.

- Pola numeryczne: Pola przechowujące liczności lub miary, np. ```vuln_total```, ```critical_count```, ```high_count```, ```component_count``` itp. powinny mieć typ integer (lub long – zależnie od spodziewanej wielkości wartości). Umożliwi to wykonywanie agregacji sum, max, min, avg na tych polach (np. średnia liczba podatności na build). Gdyby były stringami, nie można by obliczać sumy czy średniej. Podobnie, wszelkie skale (np. CVSS score, jeśli zapisywany) powinny być float lub scaled_float dla precyzji.

- Daty i czasy: Pola typu ```@timestamp``` (czas pobrania zdarzenia) lub build_time (czas wykonania builda, jeśli jest osobno) powinny być typu date. Standardowo jeśli JSON ma format czasowy rozpoznawany (np. ISO8601), Elasticsearch sam zmapuje to na date, ale warto to jawnie określić. Pozwala to korzystać z histogramów czasowych, zakresów dat itp.

- Pola zagnieżdżonych obiektów: Jeśli indeksujemy pełne listy komponentów i podatności w obrębie jednego dokumentu, zaleca się użyć typu nested dla tych list. Na przykład możemy zdefiniować mapping: ```components```: { "type": "nested", "properties": { "name": {"type": "keyword"}, "version": {"type": "keyword"}, "hash": {"type": "keyword"}, "license": {"type": "keyword"} } } oraz analogicznie ```vulnerabilities```: { "type": "nested", "properties": { "cve": {"type": "keyword"}, "severity": {"type": "keyword"}, "description": {"type": "text"} } }. Użycie nested sprawi, że Elasticsearch będzie traktował każdy element tablicy jako osobny, niezależny dokument wewnętrzny – co zapobiega pewnym niepożądanym efektom (np. dopasowaniu warunków krzyżowo między różnymi elementami tablicy przy zapytaniach bool). Innymi słowy, możemy zapytać “znajdź dokumenty, które mają jakikolwiek komponent o name=X i version=Y” i dostaniemy poprawne wyniki. Bez nested, zapytania takie mogłyby zwrócić fałszywe trafienia (gdy name i version pochodzą z różnych elementów listy). W Kibanie od wersji 7+ dostępne są też wizualizacje obsługujące pola nested (choć z pewnymi ograniczeniami), więc warto to wykorzystać, jeśli planujemy analizy wewnątrz SBOM.

- Wielkość liter: Wiele pól tekstowych (np. nazwy komponentów, identyfikatory CVE) może mieć różne formy zapisu, ale dla spójności najlepiej indeksować je ujednoliconym kluczem. Jeśli np. w SBOM nazwy zależności pochodzą z różnych ekosystemów (Java vs npm), mogą wystąpić różnice typu “React” vs “react”. Rozważ użycie normalizatora lowercase dla pól keyword, aby agregacje nie traktowały “LibX” i “libx” jako dwóch różnych wartości. Alternatywnie można to ujednolicić na etapie generowania SBOM (większość narzędzi używa lowercase names).

- Pozostałe pola: Pewne pola mogą pozostać typu text, jeśli przewidujemy potrzebę pełnotekstowego przeszukiwania. Na przykład, opisy podatności (description) można zmapować jako text (ew. z subpolem .keyword dla możliwości filtrowania po całym opisie). Dzięki temu analityk w Kibanie może wyszukać słowo kluczowe w opisach CVE (np. “deserialization”) i znaleźć wszystkie luki z nim związane. Pole z nazwą aplikacji też można dodać jako text (do wyszukiwania fragmentów nazw), ale ponieważ mamy już aid.application jako keyword do ścisłego dopasowania, nie jest to konieczne. Wreszcie, jeżeli przechowujemy hash artefaktu (np. hash obrazu Docker), to jest to ciąg, który również powinien być keyword (porównujemy cały hash).

Przykładowy fragment definicji mappingu dla indeksu sbom-test mógłby wyglądać następująco:
```json
{
    "mappings": {
        "properties": {
        "@timestamp": { "type": "date" },
        "event_type": { "type": "keyword" },
        "aid": {
            "properties": {
            "application": { "type": "keyword" },
            "version":     { "type": "keyword" },
            "build_id":    { "type": "keyword" }
            }
        },
        "components": {
            "type": "nested",
            "properties": {
            "name":    { "type": "keyword" },
            "version": { "type": "keyword" },
            "hash":    { "type": "keyword" },
            "license": { "type": "keyword" }
            }
        },
        "vulnerabilities": {
            "type": "nested",
            "properties": {
            "cve":        { "type": "keyword" },
            "severity":   { "type": "keyword" },
            "cvss_score": { "type": "float" },
            "description":{ "type": "text" }
            }
        },
        "vuln_total":      { "type": "integer" },
        "critical_count":  { "type": "integer" },
        "high_count":      { "type": "integer" },
        "medium_count":    { "type": "integer" },
        "low_count":       { "type": "integer" },
        "component_count": { "type": "integer" },
        "gate_status":     { "type": "keyword" },
        "policy_failures": { "type": "keyword" }
        }
    }
}
```
Powyżej uwzględniono kluczowe elementy: datę, typ zdarzenia, zagnieżdżone listy komponentów i podatności, pola zliczające oraz status gate. Oczywiście mapping należy dostosować do dokładnej struktury generowanych zdarzeń – ważne jest jednak, by nie polegać wyłącznie na dynamic mapping. Lepiej zawczasu określić, które pola będą potrzebne do agregacji, i ustawić je jako keywords/integer itp. (można pozostawić dynamic dla ewentualnych dodatkowych pól opisowych, ale nie dla głównych wymiarów analizy). Unikniemy w ten sposób sytuacji, że np. aid.version przez przypadek stanie się textem i nie da się pogrupować wyników per wersja.

Na koniec warto wspomnieć o rozmiarze indeksu: indeks ```sbom-test``` będzie zawierał wiele powtarzalnych wartości (np. nazwy tych samych komponentów w kolejnych buildach). Rozważenie kompresji i tzw. deduplikacji może przynieść korzyści. Elasticsearch w sposób naturalny deduplikuje wartości keyword poprzez słowniki, więc użycie keyword sprzyja oszczędności (np. słowo “log4j” będzie trzymane raz w słowniku i referencjonowane przez wiele dokumentów). Ponadto, jeśli pewne struktury są duże i rzadko używane, można je wydzielić do osobnego indeksu lub typu (np. osobny indeks z pełnymi SBOM jako dokumenty typu component-list, a główny indeks trzyma tylko referencje i agregaty). To już zaawansowane opcje optymalizacyjne zależne od wolumenu danych.

## Dalsze kierunki rozwoju ekosystemu SBOM + Elastic

Obecne rozwiązanie stanowi podstawę do dalszej rozbudowy w kierunku pełniejszej platformy DevSecOps. Poniżej omówiono potencjalne kierunki rozwoju, które mogą zwiększyć wartość analityczną i automatyzację systemu:

- Integracja z Jira (lub innym systemem ticketowym) – Celem jest zamknięcie pętli od detekcji problemu do jego obsługi przez zespół developerski. Obecnie pipeline loguje naruszenia polityk (fail gate), ale warto te informacje automatycznie przekładać na zadania do wykonania. Integracja Elastic z Jira może polegać na wyzwalaniu utworzenia ticketu, gdy pojawi się określone zdarzenie (np. krytyczna podatność). W środowisku Splunk osiąga się to akcjami alertów – analogicznie w Elastic Stack można wykorzystać mechanizm Watcher lub Alerting w Kibanie, który po wykryciu dokumentu gate FAIL z krytyczną luką wywoła webhook do API Jira. Ticket powinien zawierać kluczowe dane: ID builda, komponent i wersja sprawiająca problem, identyfikator CVE, zalecenie (np. “zaktualizować lub usunąć bibliotekę X”). Dzięki takiej integracji, gdy tylko pipeline zablokuje build, odpowiedni zespół od razu dostanie formalne zgłoszenie w swoim backlogu. Przykładem może być sytuacja z Log4Shell: pipeline wykrył CVE-2021-44228 i przerwał build – integracja spowodowała automatyczne utworzenie ticketa “Zaktualizować log4j do bezpiecznej wersji” przypisanego do zespołu aplikacji[21]. Ticket ten zawiera link do dashboardu Kibany lub szczegółów w Elastic, co ułatwia deweloperom analizę (np. mogą zobaczyć dokładnie, w którym serwisie i pliku pojawia się podatność). Taka automatyzacja gwarantuje, że nic nie “prześlizgnie się” niezauważone – każda krytyczna luka ma od razu swój rekord do śledzenia aż do zamknięcia (usunięcia luki). Dalszym rozwinięciem jest dwukierunkowa integracja: np. kiedy deweloper w Jira oznaczy ticket jako rozwiązany (po deployu fixa), system może zweryfikować w następnym pipeline czy rzeczywiście luka zniknęła, i zaktualizować ticket/linkować wynik skanu.

- Wersjonowanie SBOM (repozytorium wiedzy o komponentach) – W miarę dojrzewania procesu warto zainwestować w centralne repozytorium SBOM dla wszystkich wydań oprogramowania[20]. Choć Elasticsearch pełni tu już rolę bazy wiedzy, dedykowane repo (np. oparte o istniejące narzędzia Artefactory/Nexus lub specjalistyczne SBOM Catalogi) może zapewnić dodatkowe funkcje: przechowywanie zatwierdzonych SBOM (np. z podpisem cyfrowym), wersjonowanie i porównywanie SBOM-ów niezależnie od pipeline, możliwość udostępniania SBOM klientom lub audytorom, itp. Repo SBOM mogłoby też przechowywać komponenty z informacją o ich pochodzeniu (czy to wewnętrzna biblioteka, czy open-source, kto jest właścicielem), co pomaga w zarządzaniu ryzykiem łańcucha dostaw. Integracja takiego repo z pipeline polegałaby na tym, że po każdym buildzie SBOM jest tam publikowany (np. jako artefakt) i otagowany wersją aplikacji. W razie potrzeby audytu (np. za rok chcemy sprawdzić SBOM wersji sprzed roku) – mamy to pod ręką. Dodatkowo, posiadanie wersjonowanych SBOM-ów umożliwia budowę meta-analiz na przestrzeni całego portfolio aplikacji, co może wykraczać poza to, co wygodnie zrobimy jednym indeksem Elastic.

- Confidence Score z AI/ML – Włączenie sztucznej inteligencji do analizy SBOM może pomóc w lepszym szacowaniu ryzyka i priorytetyzacji. Przykładowo, można budować model uczący się na historycznych danych o podatnościach i ich kontekście runtime, który będzie nadawał score określający prawdopodobieństwo, że dana podatność stanowi realne zagrożenie. Taki model mógłby brać pod uwagę: czy podatna funkcja jest wykorzystywana przez aplikację (info z logów, stack trace), czy istnieje znany exploit (info z Threat Intelligence), ile czasu minęło od disclosure (czy luka jest “świeża”, a może są już łatki), jak szybko w przeszłości zespół reagował na podobne luki, itp. Na tej podstawie AI mogłaby wyznaczyć np. risk score 0-100. W praktyce Confidence Score mógłby działać podobnie do systemów typu Kenna Security – łączyć dane techniczne z kontekstem, by podpowiadać “ta podatność ma wysoki CVSS, ale nie jest używana w kodzie i brak exploitów – realne ryzyko średnie”, albo odwrotnie “tu CVSS średni, ale aktywnie exploitowana i apka korzysta z podatnej metody – realne ryzyko wysokie”. Taki score można by prezentować obok zwykłych danych na dashboardzie, co pomaga zespołom skupić się na tym, co najważniejsze. Wykorzystanie AI może też objąć np. analizę kodu źródłowego pod kątem podatności (SAST) i łączenie tego z SBOM – model mógłby wykrywać potencjalne słabe punkty nawet zanim pojawi się oficjalne CVE (przewidywanie ryzyka). Choć to kierunek eksperymentalny, trend “AI w cyber” jest silny – niewykluczone, że pojawią się gotowe usługi nadające scoring podatnościom na bazie milionów punktów danych zewnętrznych. Już dziś jednak można zacząć od integracji solidnych Threat Intelligence – np. automatycznego tagowania podatności flagą “aktywnie atakowana” gdy są doniesienia o exploitach[22], co jest formą inteligencji zwiększającej naszą pewność co do priorytetu.

- VEX (Vulnerability Exploitability eXchange) – Uzupełnieniem SBOM staje się standard VEX, który koncentruje się na przekazywaniu informacji, czy dana podatność faktycznie wpływa na produkt w konkretnym kontekście[23]. W praktyce VEX to dokument (powiązany z SBOM), w którym dostawca oprogramowania deklaruje status znanych luk: np. “CVE-2022-12345 – Not Affected (komponent obecny, ale podatna funkcja niewykorzystana)” lub “CVE-2021-98765 – Fixed (by upgrade to wersja X)”. Integracja VEX z naszym pipeline polegałaby na tym, że wyniki skanu byłyby filtrowane przez dodatkową warstwę wiedzy kontekstowej: jeżeli dla danej luki mamy wprowadzone oznaczenie “Not Affected” (np. z analizy manualnej lub od dostawcy komponentu), to pipeline może ją pominąć przy ocenie gate (nie traktować jako naruszenie). Oczywiście powinna istnieć ścisła kontrola tego mechanizmu – wpisy VEX muszą pochodzić z zaufanego źródła (np. od naszego zespołu AppSec lub vendorów). Dodanie obsługi VEX w Elastic może oznaczać posiadanie osobnego indeksu z dokumentami VEX i korelowanie go z danymi SBOM (po identyfikatorach CVE i komponentów). Przyszłościowo można wyobrazić sobie pełną automatyzację: pipeline generuje SBOM, automatycznie odpytuje bazę VEX (np. dostawcy oferują takie API) i oznacza w raporcie, które luki można odfiltrować. VEX to stosunkowo nowy element ekosystemu SBOM, ale istotny – pozwala odsiać false alarms (lukę, która jest teoretyczna, ale nie dotyczy naszego use-case’u). Włączenie go do procesu znacząco zwiększy wiarygodność alertów – zostaną tylko te podatności, które realnie trzeba adresować.

- Feedback loop do developerów – Proces DevSecOps powinien działać nie tylko jako kontrola, ale także jako system uczący organizację lepszych praktyk. Dlatego ważne jest zamknięcie pętli informacji zwrotnej dla deweloperów. Kilka mechanizmów już omówiliśmy (np. automatyczne tickety w Jira), ale można pójść dalej. Dobrym kierunkiem jest integracja wyników skanów z narzędziami, z których korzystają programiści na co dzień: system kontroli wersji i narzędzia przeglądu kodu. Przykładowo, pipeline mógłby komentować Pull Requesty, jeśli nowy kod dodaje podatną zależność – informując od razu podczas code review: “Dodajesz bibliotekę X, która ma znane CVE-xxxx, czy na pewno tego potrzebujesz?”. Innym pomysłem jest widget w repozytorium (np. odznaka w README projektu) pokazujący aktualny status bezpieczeństwa (ile podatności open, czy ostatni build bezpieczny). Celem jest uczynienie informacji SBOM widoczną i dostępną dla devów, by nie traktowali jej jako odległej, ukrytej w narzędziach security. Im szybciej deweloperzy dostaną feedback (najlepiej jeszcze zanim merge’ują kod, który zepsuje build), tym lepiej. Można także włączyć deweloperów w proces analizy – np. poprzez dostęp do dashboardów Kibany z filtrem na ich projekt, by sami mogli eksplorować dane (to wymaga odpowiednich uprawnień, by nie widzieli danych innych projektów, jeśli to wrażliwe).

Ponadto, warto mierzyć efektywność pętli feedbacku – wspomniany wcześniej KPI MTTR (mean time to remediate) dla luk jest tu kluczowy[12]. Jeśli widzimy w danych, że średnio zajmuje 30 dni załatanie krytycznej luki od momentu wykrycia w pipeline, to jest pole do usprawnienia (poprzez proces lub szkolenia). Zmniejszanie tego czasu będzie jednym z celów DevSecOps. Dane z Elastic mogą posłużyć do raportowania takich wskaźników – np. budujemy wykres “czas życia podatności” od daty pierwszego wykrycia do zaniku w skanach (wymaga korelacji na poziomie CVE per projekt). To zamyka pętlę: nie tylko wykrywamy i zgłaszamy problemy, ale też śledzimy postępy w ich rozwiązywaniu, co finalnie podnosi poziom bezpieczeństwa produktu.

Na koniec, rozszerzeniem idei feedback loop jest sprzężenie zwrotne do systemów prewencji. Skoro SBOM stał się naszym czujnikiem, można rozważyć, czy pewnych informacji nie wykorzystać automatycznie gdzie indziej – np. do systemów build cache (jeśli komponent ma znane luki, oznacz go jako tainted w katalogu artefaktów), do monitoringu runtime (jeśli aplikacja ma podatny komponent X, ustawić wzmożone logowanie lub czujność w APM dla funkcji z tego komponentu) czy nawet do mechanizmów autonomic healing (to na razie futurystyczne – ale np. dynamiczne blokowanie wywołań podatnej funkcji w runtime dopóki nie będzie patcha). Wszystkie te pomysły wpisują się w filozofię, że SBOM + analiza to żywy system nerwowy bezpieczeństwa organizacji, który nie tylko wykrywa problemy, ale też inicjuje reakcje i uczy zespoły lepszych nawyków.

Rozwijając dalej platformę w powyższych kierunkach, zbliżamy się do wizji, w której bezpieczeństwo jest ciągłe i zautomatyzowane, ale z zachowaniem kontroli przez człowieka tam, gdzie to potrzebne (np. procesy wyjątku, akceptacja ryzyka). Elastic Stack, dzięki swojej elastyczności, może stopniowo integrować kolejne źródła (Jira, TI, logi, VEX) i stać się centralnym hubem informacji o stanie aplikacji. W efekcie organizacja zyskuje proaktywność – reaguje na zagrożenia zanim staną się incydentami – oraz wiedzę – pełny obraz tego, z czego składa się jej oprogramowanie i jakie niesie to implikacje dla jakości i bezpieczeństwa. Taki poziom dojrzałości procesu stanowi już zaawansowaną formę Application Security Posture Management, gdzie SBOM jest fundamentem, a nad nim zbudowano bogaty system analityczny.

# Bibliografia: 

[1] [7] [17] [21] Akademia SBOM – kompendium wiedzy teoretycznej o analizie i budowie procesów SBOM.docx

file://file-KvX9FcBQ8VfHzdyUkXkWog

[2] [4] Najprostsza architektura integrująca Jenkins, domenę Windows i Splunk w CI_CD.docx

file://file-Ns9B1QUo6DoavjXQTtjcKJ

[3] Architektura pętli procesowej DevSecOps z SBOM i Human-in-the-Loop.docx

file://file-3E1YPSJVRsjeMWNk7jjyi7

[5] [6] [8] [9] [10] [11] [12] [16] [19] SBOM jako _Sigillum Relationis_ – ontologiczna pieczęć relacji w systemach cybernetycznych.docx

file://file-TZ8Rb6AbaVTUHhTgLHGf1q

[13] [14] [15] [22] SBOM jako strumień danych w architekturze procesowej.docx

file://file-2amRBJY9kAbutAZQLu49ji

[18] DevSecOps z analizą SBOM w CI_CD – fazy wdrożenia i zadania zespołów.docx

file://file-VirenxMyo9Kes6duVBGWMa

[20] Architektura hybrydowego rozwiązania do analizy SBOM w procesie CI_CD.docx

file://file-XGDCCVE3nStHFSG12VG4Le

[23] Vulnerability Exploitability eXchange (VEX) - CycloneDX

https://cyclonedx.org/capabilities/vex/



